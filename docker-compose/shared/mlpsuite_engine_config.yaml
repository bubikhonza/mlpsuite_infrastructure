stages:
  - tokenizer:
    - module_name: pyspark.ml.feature
    - class_name: Tokenizer
    - args:
      - inputCol=text[str]
      - outputCol=words[str]
  - hashingtf:
    - module_name: pyspark.ml.feature
    - class_name: HashingTF
    - args:
      - inputCol=words[str]
      - outputCol=features[str]
  - logistic_reg:
    - module_name: pyspark.ml.classification
    - class_name: LogisticRegression
    - args: 
      - maxIter=10[int]
      - regParam=0.001[float]

train:
  - path: /shared/train_test.csv
  - header: true
  - schema:
    - id: IntegerType
    - text: StringType
    - label: DoubleType
  - pipeline_output: /shared/models/

predict:
  - pipeline_path: /shared/models/
  - version: 202301251941_RTE
  - schema:
    - id: IntegerType
    - text: StringType

  - input_kafka_options:
      - kafka.bootstrap.servers: "kafka:9092"
      - subscribe: "input"
      - failOnDataLoss: "false"

  - output_kafka_options:
      - kafka.bootstrap.servers: "kafka:9092"
      - checkpointLocation: "/shared/checkpoint"
      - topic: "output"